#! /usr/bin/env python3

import torch
import gc
import random
from torch.autograd import grad
from LLMIF.utils import display_progress
import random

def print_memory(mes = None):
    print(mes)
    memory_allocated = torch.cuda.max_memory_allocated()
    print(f"Allocated GPU memory: {memory_allocated / 1024**3:.2f} GB")

def s_test(z_test, t_test, input_len, model, z_loader, gpu=-1, damp=0.01, scale=25.0,
           recursion_depth=5000):
    """s_test can be precomputed for each test point of interest, and then
    multiplied with grad_z to get the desired value for each training point.
    Here, strochastic estimation is used to calculate s_test. s_test is the
    Inverse Hessian Vector Product.

    Arguments:
        z_test: torch tensor, test data points, such as test images
        t_test: torch tensor, contains all test data labels
        model: torch NN, model used to evaluate the dataset
        z_loader: torch Dataloader, can load the training dataset
        gpu: int, GPU id to use if >=0 and -1 means use CPU
        damp: float, dampening factor
        scale: float, scaling factor
        recursion_depth: int, number of iterations aka recursion depth
            should be enough so that the value stabilises.

    Returns:
        h_estimate: list of torch tensors, s_test"""
    v = grad_z(z_test, t_test, input_len, model, gpu)[0]
    h_estimate = tuple(v.copy())

    ################################
    # TODO: Dynamically set the recursion depth so that iterations stops
    # once h_estimate stabilises
    ################################
    for i in range(recursion_depth):
        # take just one random sample from training dataset
        # easiest way to just use the DataLoader once, break at the end of loop
        #########################
        # TODO: do x, t really have to be chosen RANDOMLY from the train set?
        #########################
        for x, t, _, _ in z_loader:
            if gpu >= 0:
                x, t = x.cuda(gpu), t.cuda(gpu)
            print("x:", x.shape)
            with torch.cuda.amp.autocast(dtype=torch.float16):
                y = None
                y = model(x)
                y = y.logits
                loss = calc_loss(y, t)
                loss = loss.mean(dim=1)
                params = [ p for p in model.parameters() if p.requires_grad and p.dim() >= 2 ]
                hv = hvp(loss, params, h_estimate)
                # print("***hv", hv[0])

            model.zero_grad(set_to_none=True)
            torch.cuda.empty_cache()
            gc.collect()

            for _v, _h_e, _hv in zip(v, h_estimate, hv):
                if torch.isnan(_v).any() or torch.isinf(_v).any():
                    print(i, "_v", _v, _v.shape, torch.nonzero(torch.isnan(_v.view(-1))))
                    break
                if torch.isnan(_h_e).any() or torch.isinf(_h_e).any():
                    print(i, "_h_e", _h_e, _h_e.shape, torch.isinf(_h_e).any())
                    break
                if torch.isnan(_hv).any() or torch.isinf(_hv).any():
                    print(i, "_hv", _hv, _hv.shape, torch.isinf(_hv).any())
                    break

            # Recursively caclulate h_estimate
            h_estimate = [
                _v + (1 - damp) * _h_e - _hv / scale
                for _v, _h_e, _hv in zip(v, h_estimate, hv)]
#             print("h_estimate", h_estimate[0])
#             for i, _h in enumerate(h_estimate):
#                 if torch.isnan(_h).any() or torch.isinf(_h).any():
#                     print(f"h {i} has nan or inf", torch.isnan(_h).any(), torch.isinf(_h).any())
#                     print("h:", _h, _h.shape)
#                     break
            break
        display_progress("Calc. s_test recursions: ", i, recursion_depth)
    exit()
    return h_estimate


def calc_loss(y, t):
    """Calculates the loss

    Arguments:
        y: torch tensor, input with size (minibatch, nr_of_classes)
        t: torch tensor, target expected by loss of size (0 to nr_of_classes-1)

    Returns:
        loss: scalar, the loss"""
#     # Shift so that tokens < n predict n
#     y = y[..., :-1, :].contiguous()
#     t = t[..., 1:].contiguous()

    bs, _, vocab_size = y.shape
    y = y.reshape(-1, vocab_size)
    t = t.reshape(-1)

    loss = torch.nn.functional.cross_entropy(y, t, reduction='none')
    loss = loss.reshape((bs, -1))
    return loss


def grad_z(z, t, input_len, model, gpu=-1):
    """Calculates the gradient z. One grad_z should be computed for each
    training sample.

    Arguments:
        z: torch tensor, training data points
            e.g. an image sample (batch_size, 3, 256, 256)
        t: torch tensor, training data labels
        model: torch NN, model used to evaluate the dataset
        gpu: int, device id to use for GPU, -1 for CPU

    Returns:
        grad_z: list of torch tensor, containing the gradients
            from model parameters to loss"""
    model.eval()
    # initialize
    if gpu >= 0:
        z, t = z.cuda(gpu), t.cuda(gpu)
    with torch.cuda.amp.autocast(dtype=torch.float16):
        y = None
        y = model(z)
        y = y.logits
        loss = calc_loss(y, t)
        loss = loss.mean(dim=1)
        # Compute sum of gradients from model parameters to loss
        params = [ p for p in model.parameters() if p.requires_grad and p.dim() >= 2]
        return list(list(grad(l, params)) for l in loss)


def hvp(y, w, v):
    """Multiply the Hessians of y and w by v.
    Uses a backprop-like approach to compute the product between the Hessian
    and another vector efficiently, which even works for large Hessians.
    Example: if: y = 0.5 * w^T A x then hvp(y, w, v) returns and expression
    which evaluates to the same values as (A + A.t) v.

    Arguments:
        y: scalar/tensor, for example the output of the loss function
        w: list of torch tensors, tensors over which the Hessian
            should be constructed
        v: list of torch tensors, same shape as w,
            will be multiplied with the Hessian

    Returns:
        return_grads: list of torch tensors, contains product of Hessian and v.

    Raises:
        ValueError: `y` and `w` have a different length."""
    if len(w) != len(v):
        raise(ValueError("w and v must have the same length."))

    # First backprop
    first_grads = grad(y, w, create_graph=True)

    # Elementwise products
    elemwise_products = 0
    for i, _v in enumerate(first_grads):
        if torch.isnan(_v).any() or torch.isinf(_v).any():
            print(f"first_grads {i} has nan or inf", torch.isnan(_v).any(), torch.isinf(_v).any())
            print("first_grads:", _v)
            break
    for i, _v in enumerate(v):
        if torch.isnan(_v).any() or torch.isinf(_v).any():
            print(f"v {i} has nan or inf", torch.isnan(_v).any(), torch.isinf(_v).any())
            print("v:", _v)
            break
    for grad_elem, v_elem in zip(first_grads, v):
        elemwise_products += torch.sum(grad_elem * v_elem)
#     print("w", w[0].type())
    print("elemwise_products", elemwise_products, elemwise_products.type())
#     for i, _w in enumerate(w):
#         if torch.isnan(_w).any():
#             print(f"w {i} has nan")
#             print("w:", _w)
#     print("-"*50)
    #  elemwise_products = elemwise_products.half()

    # Second backprop
    return_grads = grad(elemwise_products, w)
    # print("return_grads:", return_grads)
    # return_grads = [x.half() for x in return_grads]
    # del first_grads

    return return_grads
